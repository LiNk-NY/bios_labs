---
title: "Predicted Probabilities and ROC"
author: "BIOS 620"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output:
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Setup: ISLR package

Intro to Statistical Learning

```{r}
library(ISLR)
data(Default, package = "ISLR")
head(Default)
```

## Default data documentation

```{r,eval=FALSE}
?Default
```

## Specify the Model

```{r}
class(Default$default)
contrasts(Default$default)
fit <- glm(default ~ 1 + student + balance + income,
    family="binomial", data=Default)
summary(fit)
exp(coef(fit))
```

For example, if we had a second model, we could include in the ROC plot to
compare against the first.

```{r}
fit2 <- glm(default ~ balance + income, family = "binomial", data = Default)
nostudent.p <- predict(fit2, type = "response")
```


### Histogram of predicted probabilities

```{r}
p.default <- predict(fit, type = "response")
hist(p.default)
```

### Calculating sensitivity and specificity

* Predicted by Truth

|                              | Condition positive | Condition negative |
|------------------------------|--------------------|--------------------|
| Predicted condition positive | True Positive      | False Positive     |
| Predicted condition negative | False Negative     | True Negative      |

* TP = True Positive
* FN = False Negative

* specificity = TN / ( TN + FP )
* sensitivity  = TP / ( TP + FN )

```{r}
threshold.seq <- seq(0, 1, by = 0.01)
sens <- spec <- rep(NA, length(threshold.seq))

for (i in seq_along(threshold.seq)) {

  decision <- as.numeric(p.default > threshold.seq[i])
  myt <- table(decision, Default$default)
  a <- sum(decision==0 & Default$default == "No")
  d <- sum(decision==1 & Default$default == "Yes")

  spec[i] <- a / sum(myt[, "No"]) # (myt[1,1]+myt[2,1])
  sens[i] <- d / sum(myt[, "Yes"]) # (myt[1,2]+myt[2,2])

}

spec.sens <- data.frame(specificity = spec, sensitivity = sens, cutoff = threshold.seq)
head(spec.sens)
dim(spec.sens)
```

## ROC curve

```{r}
plot(1 - spec.sens[, "specificity"], spec.sens[, "sensitivity"],
    xlim = c(0, 1), ylim = c(0, 1), lwd = 2,
    type = "l", col = "red", main = "ROC curve",
    xlab = "1 - Specificity", ylab = "Sensitivity", asp = 1)
abline(a=0, b=1)
```

# Alternative calculations

* Using lowess smoothing (local averaging)
* Using True Positive Rate and False Positive Rate

```{r}
coff <- unique(p.default)
tp <- fp <- rep(NA, length(coff))

for (i in seq_along(coff)) {
  def.y <- as.numeric(p.default >= coff[i])
  xtab <- table(def.y, Default$default)

  tp[i] <- xtab["1", "Yes"] / sum(xtab[, "Yes"])
  fp[i] <- xtab["1", "No"] / sum(xtab[, "No"])
}
```

We can calculate the True Positive Rate and False Positive Rate for Model 2:

```{r}
coff.s <- unique(nostudent.p)
tp.s <- fp.s <- rep(NA, length(coff.s))

for (i in seq_along(coff.s)) {
  def.y <- as.numeric(nostudent.p >= coff.s[i])
  xtab <- table(def.y, Default$default)

  tp.s[i] <- xtab["1", "Yes"] / sum(xtab[, "Yes"])
  fp.s[i] <- xtab["1", "No"] / sum(xtab[, "No"])
}
```


## Plotting the ROC curve

Here we create the plotting space:
```{r}
plot(c(0, 1), c(0, 1), type = "n", axes = FALSE, xlab="", ylab = "", asp = 1)
axis(1, pos = 0.0)
axis(2, pos = 0.0, las = 1)
mtext("False Positive Rate", side = 1, line = 1)
mtext("True Positive Rate", side = 2, line = 1)
segments(x0 = c(0, 0, 1), x1 = c(1, 1, 1), y0 = c(0, 1, 0), y1 = c(1, 1, 1))
```

Now we add the line for Model 1:

```{r}
lowe1 <- lowess(x = fp, y = tp, f = 1/10)
lines(x = lowe1$x, y = lowe1$y, lwd = 2, col="red")
```

Add the line for model 2:

```{r}
lowe2 <- lowess(x = fp.s, y = tp.s, f = 1/10)
lines(x = lowe2$x, y = lowe2$y, lwd = 2, col="blue")
```

```{r}
legend(0.65, 0.2, legend = c("Model 1", "Model 2"),
       col = c("red", "blue"), lty = 1, lwd = 2)
```


As you can see, both lines are quite similar. This is due to the predictive
ability of the variable that was dropped (`student`).

## Calculating the Area Under the Curve (AUC)

We can compare models by their AUC. It is a measure of how well the model
performs in predictive ability.

```{r}
len <- length(lowe1$x)
h <- lowe1$x[-1] - lowe1$x[-len]   # heights of individual parallelograms
## alternatively
## h <- diff(lowe$x)

ub <- lowe1$y[-1]
lb <- lowe1$y[-len]
auc1 <- sum( ( (ub + lb) * h )/2 )
```

```{r}
len <- length(lowe2$x)
h <- lowe2$x[-1] - lowe2$x[-len]   # heights of individual parallelograms
## alternatively
## h <- diff(lowe$x)

ub <- lowe2$y[-1]
lb <- lowe2$y[-len]
auc2 <- sum( ( (ub + lb) * h )/2 )
```

Here are the AUC values side-by-side:

```{r}
c(model1 = auc1, model2 = auc2)
```

# Exercise

* Use the `admit.sav` data, fit a prediction model with "admit" as the response
variable to predict being admitted ("admit" = 1), using `gre`, `gpa`, `rank`,
and the interaction of `gre` and `gpa` as predictors.

* Obtain the predicted probabilities

* for each value of the threshold vector
  * calculate two vectors, specificity and sensitivity

```{r}
threshold <- seq(0, 0.75, by = 0.01)
```

* plot "1-spec vs sens" (the ROC curve)
