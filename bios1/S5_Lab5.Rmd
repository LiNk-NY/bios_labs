---
title: "OLS and MLE"
author: "BIOS 620"
output:
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Example dataset `mtcars`

```{r}
data("mtcars")
head(mtcars)
tail(mtcars)
dim(mtcars)
class(mtcars)
names(mtcars)
rownames(mtcars)
```

## Simple linear Regression

This is how equation notation in R relates to the lecture material.

```{r}
Y ~ 1 + x1 + x2
# Y_hat = (Intercept) + Beta_x1 + Beta_x2
```

```{r}
fit1 <- lm(mpg ~ 1 + qsec, data = mtcars)
summary(fit1)
```

See the difference between using `~ 1` and not with the `model.matrix` function
(i.e., there is no difference).

```{r}
head(model.matrix(lm(mpg ~ 1 + qsec, mtcars)))
head(model.matrix(lm(mpg ~ qsec, mtcars)))
```

## Draw a scatterplot

```{r}
with(mtcars, plot(qsec, mpg))
## also
# plot(mtcars$qsec, mtcars$mpg)
```

## Predicted values from the regression (yhat)

```{r}
yhat0 <- predict(fit1)
yhat1 <- fitted(fit1)
yhat2 <- coef(fit1)[1] + coef(fit1)[2] * mtcars$qsec

hatdf <- data.frame(yhat0, yhat1, yhat2)
head(hatdf)
```

## Dummy Variables

* Only needed for SPSS and SAS
* Only make `k` - 1 categories
* Category that is left out is the reference

```{r}
library(haven)
nhanes <- read_spss("NHANES_Lab4.sav")
head(nhanes, 3)
nhanes$Edu_f <- as_factor(nhanes$Education)
table(nhanes$Edu_f)

nhanes$hsGrad <- ifelse(
   nhanes$Edu_f == "Graduated from HS (but not college)", 1, 0
)
nhanes$College <- ifelse(
   nhanes$Edu_f == "Graduated from College", 1, 0
)
# as.numeric(nhanes$Edu_f == "Graduated from College")
table(nhanes$hsGrad)
table(nhanes$College)

head(nhanes)

lm(BMXBMI ~ hsGrad + College, data = nhanes)
# preferred method
lm(BMXBMI ~ Edu_f, data = nhanes)

lm(BMXBMI ~ factor(Education), data = nhanes)
```


## Adding a regression line to our scatterplot

```{r}
with(mtcars, plot(qsec, mpg))
lines(mtcars$qsec, yhat0, col = "yellow", lty = 3, lwd = 3)
abline(fit1, col = "green", lty = 2)
```

To see these two vectors side by side, we can use the `cbind` function or
`data.frame`.

```{r}
head(cbind(qsec = mtcars$qsec, yhat0))
# head(data.frame(mtcars$qsec, yhat0))
```

## Manual calculation of TSS, RegSS, and ESS

```{r}
y_mean <- mean(mtcars$mpg)
y_mean
```

```{r}
TSS <- sum((mtcars$mpg - y_mean)^2)
TSS
```

```{r}
RegSS <- sum((yhat0 - y_mean)^2)
RegSS
```

```{r}
ESS <- sum((mtcars$mpg - yhat0)^2)
```

```{r}
RegSS + ESS
```

# Manually calculate the coefficients

\[
\begin{split}
   \hat{\beta}_{1} &= \frac{\sum_{i=1}^{n}(x_{i} - \bar{x})(y_{i} - \bar{y})}{\sum_{i=1}^{n}(x_{i} - \bar{x})^2} \\
   \hat{\beta}_{0} &= \bar{y} - \hat{\beta}_{1} \bar{x}.\\
\end{split}
\]

```{r}
x_mean <- mean(mtcars$qsec)
```

```{r}
x <- mtcars$qsec
y <- mtcars$mpg
beta1 <- sum ((x - x_mean) * (y - y_mean)) / sum ((x - x_mean)^2 )
beta1
```

```{r}
beta0 <- y_mean - (beta1 * x_mean)
beta0
```

```{r}
coef(fit1)
```

```{r}
beta0
beta1
```

# Maximum Likelihood Estimation

See these links for digestible explanations of the method:

https://alemorales.info/post/mle-nonlinear/

https://rpsychologist.com/likelihood/

Essentially, we want to maximize the probability of observing the values
that we have by choosing the optimal parameters (e.g., normal dist.: mean and
variance) for a given distribution. We use the computer to iterative over
a number of parameters until it finds the parameters where the likelihood
is highest given our data.

# Lab 5 (empirically prove formula and equation)

Fit a regression with only 1 covariate (either by simulation or real data).

* Fit a linear regression model to obtain the regression coefficients (intercept
and slope) by using `lm()` function.

* Manually calculate the coefficients using the formula below and compare with
the model's output

\[
\begin{split}
   \hat{\beta}_{1} &= \frac{\sum_{i=1}^{n}(x_{i} - \bar{x})(y_{i} - \bar{y})}{\sum_{i=1}^{n}(x_{i} - \bar{x})^2} \\
   \hat{\beta}_{0} &= \bar{y} - \hat{\beta}_{1} \bar{x}.\\
\end{split}
\]

* Calculate the
LHS (left hand side) of following equation to see if the resulting numerical
value is close to zero.

\[
  \sum_{i=1}^{n} (\hat{y}_{i} - \bar{y})(y_{i} - \hat{y}_{i}) = 0.
\]
